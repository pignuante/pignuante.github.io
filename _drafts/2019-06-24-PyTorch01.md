---
layout: post
title: 인공신경망
description: ""
date: 2019-06-24
tags: [python, pytorch, visdom]
comments: true
---



### 0. 들어가며

### 1. 신경망이란 무엇인가

![인공신경망](https://github.com/pignuante/pignuante.github.io/blob/master/assets/images/Pytorch_ann/20130708170009.jpg?raw=true)

<center>그림 1. 인공신경망</center>
인공신경망(Artificial Neural Networks; ANN)은  McCulloch, W.S., Pitts, W (1943)에서 처음 소개 되었다. 생물학적 신경망은 여러 자극이 수상돌기(Dendrite)들을 통하여 신경세포로 넘어오고 어느정도 이상의 자극이 들어오면 이를 축색?삭?(Axon)을 통해 다른 세포로 전하는 구조이다. 이러한 구조를 프로그래밍적으로 모방하여 만든 것이 ANN이다. 여러 자극 즉, 입력이 들어오면 각각의 가중치를 곱하여 더해주고 추가적으로 편차도 더해주어서 활성화 함수(Activation Function)을 통해 변형하여 전달하는 단위를 인공 뉴런(Artificial Neuron)이라한다. 이러한 뉴런들이 모인 네트워크를 ANN이라 한다. 

![인공신경망의 발전사](https://github.com/pignuante/pignuante.github.io/blob/master/assets/images/Pytorch_ann/ANN_발전사.jpeg?raw=true)

<center>그림 2. ANN의 발전사</center>
ANN은 명제 논리(Propositional Logic)를 사용하여 생명체의 뇌의 생물학적 뉴런이 복잡한
계산을 위해 어떻게 상호작용하는지에 대해서 계산 모델을 제시하였다. 1960년대까지는 ANN을 통해서 지능을 가진 기계화 대화가 가능 할 것이라고 사람들은 생각하였다.  하지만 사람들의 기대와는 달리 XOR 문제를 해결할 수 없고, 1990년 대에는 서포트 벡터 머신(Support Vector Machine; SVM)과 같은 성능 좋은 기계학습 알고리즘이 개발되면서 ANN의 사용은 줄어 들게 된다. 하지만 2000년대에 들면서 ANN은 하드웨어의 성능의 비약적인 발전으로 납득할만한 시간에 대규모의 신경망을 학습 할 수 있게 되었으며 훈련 알고리즘의 지속적인 발달로 약진할 수 있게 되었다.



### 3. 인공 신경망의 요소

![단층 신경망](https://github.com/pignuante/pignuante.github.io/blob/master/assets/images/Pytorch_ann/Perceptron.png?raw=true)

<center>그림 3. 단층 인공신경망</center>
![다층 인공신경망](https://github.com/pignuante/pignuante.github.io/blob/master/assets/images/Pytorch_ann/multilayer.png?raw=true)

<center>그림 4. 심층 인공신경망</center>
인공신경망은 여러 개의 입력 값과 출력 값을 가질 수 있다. 입력단과 출력단 사이의 은닉층(Hidden Layer)의 개수에 따라서 단층 신경망과 심층 신경망으로 나뉜다. 심층 신경망은 2개 이상의 은닉층을 가진 신경망을 의미한다. 

![가중치](https://github.com/pignuante/pignuante.github.io/blob/master/assets/images/Pytorch_ann/perceptron_.png?raw=true)

<center>그림 5. 가중치의 곱</center>


입력값들의 가중치의 합을 활성화 함수에 통과시켜 변형시키고 이 과정을 모든 값에 적용시켜 최종 결과값을 만들어 낸다. _그림 4_의 네트워크를 수식으로 나타내면 다음과 같다.

$$y=w_4\left( \sigma\left( w_3\left( \sigma\left( w_2\left( \sigma\left( w_1\times x+b_1 \right) \right)+b_2 \right) \right)+b_3 \right) \right)+b_4$$

이렇게 계산된 값들은 시그모이드(Sigmoid), 하이퍼 볼릭 탄젠트($$\tanh$$), 수정 선형 유닛(Rectified Linear Unit; ReLU)같은 활성화 함수를 거쳐서 **비선형성**(Nonlinearity)를 띄도록 만든다. 이런 비선형성으로의 변화를 거치지 않을 경우 은닉층이 몇 개라도 결국 선형변환이기 때문에 깊은 모델을 만든 의미가 사라진다. 활성화 함수에는 다양한 종류가 있는데 많이 사용되는 활성화 함수의 종류는 _그림 6_과 같다. 보통 인공 신경망에서 많이 사용되는 활성화 함수는 Sigmoid와 $$\tanh$$이다. 시그모이드는 소위 통계학에서 말하는 로짓 변환을 사용한 로지스틱(Logisitic) 함수 $$\frac{1}{1+e^{-x}}$$, 하이퍼 볼릭 탄젠트는 $$\frac{e^{x} - e^{-x}}{e^{x}+e^{-x}}$$ 의 형태를 가진다.

![활성화 함수](https://github.com/pignuante/pignuante.github.io/blob/master/assets/images/Pytorch_ann/%20activation_function.png?raw=true)

<center>그림 6. 다양한 활성화 함수</center>


### 4. 전파와 역전파

인공신경망은 입력값이 들어오면 여러 개의 은닉층을 거쳐서 결과값을 만들어내는데, 이러한 과정을 순전파(Feed Forward)라고 한다.



$$ \left[
\begin{array}{cccc}
  	w_{00} & w_{01} & w_{02} &  w_{02}\\ 
	w_{10} & w_{11} & w_{12} &  w_{12} \\ 
	w_{20} & w_{21} & w_{22} &  w_{22} \\ 
	w_{30} & w_{31} & w_{32} &  w_{32}
\end{array}
\right] 
\left[
\begin{array}{cccc}
  	w_{00} & w_{01} & w_{02} &  w_{02}\\ 
	w_{10} & w_{11} & w_{12} &  w_{12} \\ 
	w_{20} & w_{21} & w_{22} &  w_{22} \\ 
	w_{30} & w_{31} & w_{32} &  w_{32} \\ 
	w_{40} & w_{41} & w_{42} &  w_{42}
\end{array}
\right]
\left[
\begin{array}{cc}
  	w_{00} & w_{01}\\ 
	w_{10} & w_{11}\\ 
	w_{20} & w_{21}\\ 
	w_{30} & w_{31}\\ 
	w_{40} & w_{41}
\end{array}
\right]   $$

간단한 행렬의 곱이다 $$3\times 4$$ 와 $$4\times4$$ 과 $$4\times2$$ 의 결과는 $$3\times 2$$ 이다. 
우리는 [5, 3] 형태의 입력을 가정한다. 5는 데이터의 개수, 3은 데이터의 Feature의 개수이다.
 $$I$$ 는 입력(Input),  $$w$$ 는 가중치(Weight) 그리고 $$o$$ 는 출력(Output)이다.

$$ \left[
\begin{array}{ccc}
  	I_{00} & I_{01} & I_{02}\\ 
	I_{10} & I_{11} & I_{12}\\ 
	I_{20} & I_{21} & I_{22}\\ 
	I_{30} & I_{31} & I_{32}\\ 
	I_{40} & I_{41} & I_{42}
\end{array}
\right] 
\left[
\begin{array}{cccc}
  	w_{00} & w_{01} & w_{02} &  w_{02}\\ 
	w_{10} & w_{11} & w_{12} &  w_{12} \\ 
	w_{20} & w_{21} & w_{22} &  w_{22} \\ 
	w_{30} & w_{31} & w_{32} &  w_{32}
\end{array}
\right]=
\left[
\begin{array}{cccc}
  	o_{00} & o_{01} & o_{02} &  o_{02}\\ 
	o_{10} & o_{11} & o_{12} &  o_{12} \\ 
	o_{20} & o_{21} & o_{22} &  o_{22} \\ 
	o_{30} & o_{31} & o_{32} &  o_{32} \\ 
	o_{40} & o_{41} & o_{42} &  o_{42}
\end{array}
\right]      $$



$$\begin{align}
 		O_{00}=I_{00}\times w_{00} + I_{01}\times w_{10} + I_{02}\times w_{20} \\ 
		O_{32}=I_{30}\times w_{02} + I_{31}\times w_{12} + I_{32}\times w_{22}
\end{align}$$

의 결과로 계산이 된다. 
예측값인 $$\hat{y}$$ 를 구하는 과정이 바로 전파이다. 가중치를 $$w$$, 편차를 $$b$$ 그리고 활성화 함수를 $$\sigma$$ 로 두고 전파의 과정을 수식으로 표현하면 다음과 같다.\
$$\hat{y} = w_{3}\times \sigma\left( w_{2} \times \sigma\left( w_{1} \times x + b_{1} \right)+b_{2} \right)+b_{3}$$



이제는 손실(Loss)를 계산 할 차례이다. 정답을 $$y$$ 라 가정하였을 때 가중치과 편차에 대한 손실을 미분하여보면 다음식과 같다.

$$\begin{align}
	\mbox{loss}\ = &\ \hat{y} - y = w_{3}\times \sigma\left( w_{2} \times \sigma\left( w_{1} \times x + b_{1} \right)+b_{2} \right)+b_{3} - y \\ 
	\frac{\delta\mbox{loss}}{\delta w_{3}} = &\ \sigma\left(w_{2}\times \sigma\left( w_{1}\times x + b_{1}\right)+b_{2}\right) \\ 
	\frac{\delta \mbox{loss}}{\delta b_{3}} = &\ 1
\end{align}$$

의 기울기를 가지게 된다. 그러면 $$\frac{\delta \mbox{loss}}{\delta w_{2}}$$의 미분은  미적분에서 사용되는 연쇄법칙(Chain Rule)을 사용한다.
$$\begin{align} 
\Delta z =&\ \frac{\delta z}{\delta y}\Delta y\\ 
\Delta y =&\ \frac{\delta y}{\delta x}\Delta x \\ 
\Delta z =&\ \frac{\delta z}{\delta y}\frac{\delta y}{\delta x} \Delta x \\ 
\frac{\delta z}{\delta x} =& \frac{\delta z}{\delta y}\frac{\delta y}{\delta x}
\end{align}$$ 

의 식을 가진다. 

우리가 가정한 식인 $$loss$$에서 $$w_{2}\times \sigma\left(w_1 \times x + b_{1}\right)$$ 은 은닉층의 입력($$H_{2\ in}$$)이며 활성화 함수를 거친 $$\sigma \left(w_{2}\times \sigma\left(w_{1}\times x + b_{1}\right)+b_{2}\right)$$는 은닉층의 출력($$H_{2\ out}$$)이다. 따라서 가중치 2($$w_{2}$$)로 미분한 결과는 다음식으로 구할 수 있다.
$$ \frac{\delta\mbox{loss}}{\delta w_{2}}= w_{3} \times \sigma’\left(H_{2\ in}\right)\times \sigma\left(w_{1} \times x + b_{1}\right)$$

이렇게 입력값이 은닉층얼 거쳐 결과로 나오는 과정이 순전파(Feed Forward)였다고 하면, **역전파**(Back Propagation)은 정갑과 결과의 차이로 계산된 손실을 연쇄법측을 이용하여 입력단까지 절달하는 과정을 의미한다.

