---
layout: post
title: "Keras RNN & LSTM 구현"
description: "Keras RNN & LSTM"
date: 2018-08-15
tags: [python, keras, rnn, lstm, dnn, deep learning]
comments: true
---

[TOC]

# Keras 튜토리얼 (3)

## RNN & LSTM구현하기

- RNN과 LSTM은 안다는 전제하에 keras 문법 활용 레시피 위주로 한다.

## 1. LSTM (Long Short Term Memory model)

- [LSTM00](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) : 이 링크에서 개념을 참고하였고
- [LSTM01](https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/) : 이 곳의 설명을 참고하였다.

## 2. 구현

### 2.1. 필요 라이브러리 import

```python
import os

import numpy as np
import pandas as pd

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

import keras
from keras.models import Sequential
from keras.layers import Embedding, LSTM, RNN, Dense, Dropout, Bidirectional
from keras import losses
from keras import optimizers
from keras import activations
from keras import backend as K

from matplotlib import pyplot as plt
import seaborn as sns

sns.set()
```

- 데이터 처리
  -  ![numpy](http://www.numpy.org/_static/numpy_logo.png)[numpy](http://www.numpy.org)
  - ![pandas](http://t1.daumcdn.net/brunch/service/user/24Ej/image/ybMYsznTbOaMeHWnAH2jswWCffE.png)[pandas](https://pandas.pydata.org)
- 전처리용
  - ![scikit-learn](https://kaggle2.blob.core.windows.net/competitions/kaggle/3428/media/scikit-learn-logo.png)[scikit-learn](http://scikit-learn.org/stable/)
- 딥러닝 모델 생성
  - ![keras](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c9/Keras_Logo.jpg/150px-Keras_Logo.jpg) [keras](https://keras.io) 
- 시각화
  - ![matplotlib](https://user-images.githubusercontent.com/21988376/35551169-c06eac26-05af-11e8-9621-dd6e22d0839b.png) [matplotlib](https://matplotlib.org/#)
  - ![seaborn](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTrZ90C8tXVYPh_O938C0sw-r3y4fgPoohguCcbyfBzfgnCLlqZiQ) [seaborn](https://seaborn.pydata.org)





### 2.2. 데이터 확인

- 가지고 있는 시계열 데이터로 예제를 들어보려했으나 우선은 삼각함수를 이용해본다.

#### 2.2.1. 데이터 생성

```python
data = np.sin(np.arange(6400)*(20*np.pi/1000))
print(data)

# 결과
[ 0.          0.06279052  0.12533323 ... -0.18738131 -0.12533323
 -0.06279052]
```

#### 2.2.2 데이터 확인

```python
plt.figure(figsize=(20,6))
sns.lineplot(data=data)
plt.title("삼각함수 LSTM test")
plt.show()
```

![sin](https://github.com/pignuante/pignuante.github.io/blob/master/assets/images/keras/lstm/00sin.png?raw=true)

#### 2.2.3. 전처리

```python
from sklearn.preprocessing import MinMaxScaler

print(data.shape)
data = data.reshape(-1, 1)
print(data.shape)

Scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = Scaler.fit_transform(data)
scaled_data = pd.DataFrame(scaled_data, columns=["data"])
# 결과
(3200,)
(3200, 1)
```

- LSTM에서 사용되는 shape으로 변경해준다
  - (**time_step**, **input_feature**)
- LSTM은 들어오는 숫자타입에 민감하다고 한다(sigmoid, tanh를 사용할때나 특히).
- 따라서 숫자의 범위를 0~1사이로 고정시킨다.

#### 2.2.4. train, test set 분리

```python
time_step = 14

for shift in range(1, time_step):
    scaled_data["shifted_{:02}".format(
        time_step-shift)] = scaled_data["data"].shift(shift)
scaled_data = scaled_data[scaled_data.columns[::-1]]
print(scaled_data.shape)

train, test = train_test_split(scaled_data.dropna(), test_size=0.20, shuffle=False)

print(train.shape)
print(test.shape)

x_train = train.drop("data", axis=1).values
y_train = train[["data"]].values
x_test = test.drop("data", axis=1).values
y_test = test[["data"]].values

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)

print(x_train.shape)
print(x_test.shape)
#
(6400, 5)
(5116, 5)
(1280, 5)

(5116, 4)
(5116, 1)
(1280, 4)
(1280, 1)

(5116, 4, 1)
(1280, 4, 1)
```

- pandas DataFrame의 shift함수로 슬라이드 시킨다.
- Scikit-learn의  **train_test_split**함수로 train set과 test set을 나눈다. 위의 코드는 8:2로 나누었다.
- 그 후 **x**를 LSTM의 feed형태로 **(Batch_size, Time_step, feature)** 의 형태로 만들어 준다.

## 2.3. LSTM 모델 생성

#### 2.3.1. simple LSTM

```python
class simpleLSTM(Sequential):
    def __init__(self, input_shape=(None, 1), time_step=14, hidden=128, dropout=0.2):
        super().__init__()
        self.add(LSTM(hidden, input_shape=input_shape, name="simpleLSTM00", go_backwards=True))
        self.add(Dropout(dropout, name="dropout00"))
        self.add(Dense(1, activation="relu"))
        self.compile(loss=losses.mse, optimizer="Nadam", metrics=["acc", rmse])
```

- 생성자
  - 한번의 epoch에 time_step의 개수를 가진 1개의 data가 들어간다.
  - hidden layer는 128개.
  - Dropout의 비율은 0.2이다.
  - 위의 코드에서 loss 함수는 **Mean squared error**를 사용
  - optimizer는 **Nadam**을 사용한다.

#### 2.3.2 모델의 실행

```python
def rmse(y_true, y_pred):
    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))

class CustomHistory(keras.callbacks.Callback):
    def init(self):
        self.train_loss = []
        self.val_loss = []
        self.acc = []
        self.val_acc = []
        self.rmse = []
        self.val_rmse = []

    def on_epoch_end(self, batch, logs={}):
        self.train_loss.append(logs.get('loss'))
        self.val_loss.append(logs.get('val_loss'))
        self.acc.append(logs.get("acc"))
        self.val_acc.append(logs.get("val_acc"))
        self.rmse.append(logs.get("rmse"))
        self.val_rmse.append(logs.get("val_rmse"))
```

```python
custom_hist = CustomHistory()
custom_hist.init()

lstm = simpleLSTM()
lstm.summary()

# 결과
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
simpleLSTM00 (LSTM)          (None, 128)               66560     
_________________________________________________________________
dropout00 (Dropout)          (None, 128)               0         
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 129       
=================================================================
Total params: 66,689
Trainable params: 66,689
Non-trainable params: 0
_________________________________________________________________
```

```python
hist = lstm.fit(x_test, y_test, epochs=300, batch_size=64, callbacks=[custom_hist], validation_split=0.2, shuffle=False)

# 결과
Train on 500 samples, validate on 126 samples
Epoch 1/100
500/500 [==============================] - 2s 4ms/step - loss: 0.3750 - acc: 0.0100 - rmse: 0.5000 - val_loss: 0.3079 - val_acc: 0.0159 - val_rmse: 0.4309
Epoch 2/100
500/500 [==============================] - 1s 2ms/step - loss: 0.3750 - acc: 0.0100 - rmse: 0.5000 - val_loss: 0.3079 - val_acc: 0.0159 - val_rmse: 0.4309

...

Epoch 99/100
500/500 [==============================] - 1s 2ms/step - loss: 0.3750 - acc: 0.0100 - rmse: 0.5000 - val_loss: 0.3079 - val_acc: 0.0159 - val_rmse: 0.4309
Epoch 100/100
500/500 [==============================] - 1s 2ms/step - loss: 0.3750 - acc: 0.0100 - rmse: 0.5000 - val_loss: 0.3079 - val_acc: 0.0159 - val_rmse: 0.4309
CPU times: user 2min 39s, sys: 16.7 s, total: 2min 56s
Wall time: 1min 42s
```

#### 2.2.3 학습과정 확인

```python
def draw_train_step(history, figsize=(16,6)):
    fig, ax00 = plt.subplots(figsize=figsize)
    ax01 = ax00.twinx()
    ax00.plot(history.train_loss, label="train_loss")
    ax00.plot(history.val_loss, label="val_loss")
    
    ax01.plot(history.acc, "r-.", label="train_acc", )
    ax01.plot(history.val_acc, "g-.", label="val_acc")
    
    ax00.set_xlabel("epoch")
    ax00.set_ylabel("loss")
    ax01.set_ylabel("acc")

    ax00.legend(loc="upper left")
    ax01.legend(loc="upper right")
    
    ax01.grid(False)
    
    plt.show()

draw_train_step(custom_hist)
```

![학습과정](https://github.com/pignuante/pignuante.github.io/blob/master/assets/images/keras/lstm/01검정.png?raw=true)

- 사실 썩 좋지 못한 학습과정이다...

#### 2.2.4. 모델 검정

```python
loss, accuracy, rmse = lstm.evaluate(x_test, y_test, batch_size=100)

print("loss     : {}".format(loss))
print("accuracy : {}".format(accuracy))
print("rmse     : {}".format(rmse))

# 결과 
626/626 [==============================] - 0s 82us/step
loss     : 3.0072346982805016e-05
accuracy : 0.020766772793980832
rmse     : 0.004876310623938235
```

- 우울한 결과치이다.

#### 2.2.5 결과 확인

```python
look_ahead = len(x_test)
xhat = x_test[0]
predictions = np.zeros((look_ahead, 1))
for i in range(look_ahead):
    prediction = lstm.predict(np.array([xhat]), batch_size=1)
    predictions[i] = prediction
    xhat = np.vstack([xhat[1:], prediction])

plt.figure(figsize=(12, 5))
plt.plot(np.arange(look_ahead), predictions, 'r', label="prediction")
plt.plot(np.arange(look_ahead), y_test[:look_ahead], label="test function")
plt.legend()
plt.show()
```



![결과00](https://github.com/pignuante/pignuante.github.io/blob/master/assets/images/keras/lstm/02결과.png?raw=true)

- 결과는 의외로 잘 나왔다.

----------------

## 3.1 simple Stacked LSTM

#### 3.1.1. 모델 구성

```python
class simpleStackedLSTM(Sequential):
    def __init__(self, stacked=3, input_shape=(None, 1), time_step=14, hidden=32, dropout=0.05):
        super().__init__()
        self.add(LSTM(hidden, input_shape=input_shape,
                      return_sequences=True,  # 출력 설정
                      name="simpleLSTM_{:02}".format(0)))
        self.add(Dropout(dropout, name="Dropout_{:02}".format(0)))
        
        for i in range(1, stacked):
            self.add(LSTM(hidden, return_sequences=True, name="simpleLSTM_{:02}".format(i)))
            self.add(Dropout(dropout, name="Dropout_{:02}".format(i)))
            
        self.add(LSTM(32, name="simpleLSTM_{:02}".format(stacked)))   
        self.add(Dropout(dropout, name="dropout_FIN"))
        
        self.add(Dense(1, activation="relu", name="OutPut"))
        self.compile(loss=rmse, optimizer="Nadam", metrics=["acc", ])
        
custom_hist = CustomHistory()
custom_hist.init()

hidden = 32
lstm = simpleStackedLSTM(hidden=hidden)

# 결과
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
simpleLSTM_00 (LSTM)         (None, None, 32)          4352      
_________________________________________________________________
Dropout_00 (Dropout)         (None, None, 32)          0         
_________________________________________________________________
simpleLSTM_01 (LSTM)         (None, None, 32)          8320      
_________________________________________________________________
Dropout_01 (Dropout)         (None, None, 32)          0         
_________________________________________________________________
simpleLSTM_02 (LSTM)         (None, None, 32)          8320      
_________________________________________________________________
Dropout_02 (Dropout)         (None, None, 32)          0         
_________________________________________________________________
simpleLSTM_03 (LSTM)         (None, 32)                8320      
_________________________________________________________________
dropout_FIN (Dropout)        (None, 32)                0         
_________________________________________________________________
OutPut (Dense)               (None, 1)                 33        
=================================================================
Total params: 29,345
Trainable params: 29,345
Non-trainable params: 0
_________________________________________________________________
```

- 모델 설정에서 **stacked**하게 되면 첫 LSTM에 <u>input_shape</u>를 꼭 설정해주고 **return_sequences**도 True을 해줘야한다. (원래 각 LSTM셸에서 다른 셸로 결과를 전달하는 옵션인듯하다.)
- 마지막 LSTM 셸을 제외한 반복되는 LSTM에는 다음 셸로 전달하기 위한 **return_sequences** 옵션이 역시 True여야한다.

#### 3.1.2 모델 검정

```python
loss, accuracy = lstm.evaluate(x_test, y_test, batch_size=100)

print("loss     : {}".format(loss))
print("accuracy : {}".format(accuracy))

# 결과
540/540 [==============================] - 1s 1ms/step
loss     : 0.010932969619278554
accuracy : 0.02037036998404397
```

- 역시나 수치상으론 처절한 결과...



#### 3.1.3 학습 과정

```python
look_ahead = len(x_test)
xhat = x_test[0]
predictions = np.zeros((look_ahead, 1))
for i in range(look_ahead):
    prediction = lstm.predict(np.array([xhat]), batch_size=1)
    predictions[i] = prediction
    xhat = np.vstack([xhat[1:], prediction])

plt.figure(figsize=(12, 5))
plt.plot(np.arange(look_ahead), predictions, 'r-.', label="prediction")
plt.plot(np.arange(look_ahead), y_test[:look_ahead],  label="test")
plt.legend()
plt.show()
```

![학습01](https://github.com/pignuante/pignuante.github.io/blob/master/assets/images/keras/lstm/03학습과정2.png?raw=true)

#### 3.1.4 결과 확인

```python
def draw_train_step(history, figsize=(16,6)):
    fig, ax00 = plt.subplots(figsize=figsize)
    ax01 = ax00.twinx()
    ax00.plot(history.train_loss, label="train_loss")
    ax00.plot(history.val_loss, label="val_loss")
    
    ax01.plot(history.acc, "r-.", label="train_acc", )
    ax01.plot(history.val_acc, "g-.", label="val_acc")
    
    ax00.set_xlabel("epoch")
    ax00.set_ylabel("loss")
    ax01.set_ylabel("acc")

    ax00.legend(loc="upper left")
    ax01.legend(loc="upper right")
    
    ax01.grid(False)
    
    plt.show()

draw_train_step(custom_hist)
```

![확인01](https://github.com/pignuante/pignuante.github.io/blob/master/assets/images/keras/lstm/04.결과2.png?raw=true)

- 잘 학습하다가 뒤에 가서 까먹는듯하다
- .(...)



----------------------------------------

## 4.1 simple stateful Stacked LSTM

#### 4.1.1 모델 구성

```python

```



#### 4.1.2. 모델 검정

#### 4.1.3. 학습과정

#### 4.1.4. 결과 확인













